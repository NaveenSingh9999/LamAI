{"class_name": "Tokenizer", "config": {"num_words": 5000, "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n", "lower": true, "split": " ", "char_level": false, "oov_token": null, "document_count": 1, "word_counts": "{\"hi\": 1}", "word_docs": "{\"hi\": 1}", "index_docs": "{\"1\": 1}", "index_word": "{\"1\": \"hi\"}", "word_index": "{\"hi\": 1}"}}